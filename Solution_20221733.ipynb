{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b70b9fd007c4612b5b88f51add052db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbff94df6a8a49a9961544d81ab26afe",
              "IPY_MODEL_70ff9c76094c40538484d0e957791156",
              "IPY_MODEL_336d31e11ca344e780e582c63e7e54fb"
            ],
            "layout": "IPY_MODEL_111fd8f8dd4d4e909a347785b43d7c30"
          }
        },
        "bbff94df6a8a49a9961544d81ab26afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2778836b924cb5a1040a7a21919c44",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a29b7221bb4916a680cd84778710fa",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "70ff9c76094c40538484d0e957791156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3018ce91800d4d819f69ef7470e19f5e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1ad9980b3d347d3b2deb93c2bb5025e",
            "value": 231508
          }
        },
        "336d31e11ca344e780e582c63e7e54fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_067b4b776ccd4a009dd4b4139758fa61",
            "placeholder": "​",
            "style": "IPY_MODEL_5da90896f96e413fb9ddbc6bb4ee415d",
            "value": " 232k/232k [00:00&lt;00:00, 3.10MB/s]"
          }
        },
        "111fd8f8dd4d4e909a347785b43d7c30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2778836b924cb5a1040a7a21919c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a29b7221bb4916a680cd84778710fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3018ce91800d4d819f69ef7470e19f5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1ad9980b3d347d3b2deb93c2bb5025e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "067b4b776ccd4a009dd4b4139758fa61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5da90896f96e413fb9ddbc6bb4ee415d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6965aeef611e487890e26bce845bdc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2907e544ff834df3b46f02e2a62fd900",
              "IPY_MODEL_dadf1eda4823436582dffd14d9804d89",
              "IPY_MODEL_9bac4ec7165541dab9105f22e4a33380"
            ],
            "layout": "IPY_MODEL_bf525a57d6674f119dac5f323aac1c7c"
          }
        },
        "2907e544ff834df3b46f02e2a62fd900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28218aea344e4a0bb1c2bb38e3fd6905",
            "placeholder": "​",
            "style": "IPY_MODEL_95fd47ef822b4d0e9f3bba27f5eb83e6",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "dadf1eda4823436582dffd14d9804d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ea208adacc4c68a86232b207adada6",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7f903cd45dc4725a6d1414dfba0da34",
            "value": 28
          }
        },
        "9bac4ec7165541dab9105f22e4a33380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38f1fd5ab5494119935c7c408c587f45",
            "placeholder": "​",
            "style": "IPY_MODEL_d97457bced2e412fbd09726ee3415edd",
            "value": " 28.0/28.0 [00:00&lt;00:00, 776B/s]"
          }
        },
        "bf525a57d6674f119dac5f323aac1c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28218aea344e4a0bb1c2bb38e3fd6905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95fd47ef822b4d0e9f3bba27f5eb83e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ea208adacc4c68a86232b207adada6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f903cd45dc4725a6d1414dfba0da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38f1fd5ab5494119935c7c408c587f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97457bced2e412fbd09726ee3415edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8549db17b78f40338f907ca920970b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3002b80b699b435597644794472cafaa",
              "IPY_MODEL_70086406573448029fc7e9e31b944b2c",
              "IPY_MODEL_42bad065754548deafde71346a3616f6"
            ],
            "layout": "IPY_MODEL_5c11bcb452884569b58f2c7b440cd528"
          }
        },
        "3002b80b699b435597644794472cafaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4d89f23c00a4e9cafc5b8a64ccec867",
            "placeholder": "​",
            "style": "IPY_MODEL_1d13ca0b2b5b4b36a37f27ce8a1240c8",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "70086406573448029fc7e9e31b944b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1ed91e027a14541932353817d7ae22e",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d666a7dfe24b7888b961cbf0377f1c",
            "value": 570
          }
        },
        "42bad065754548deafde71346a3616f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727d39a44a7d4b6a9732063ef3404fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_fcd90e9c01484d8583657042caea8579",
            "value": " 570/570 [00:00&lt;00:00, 17.4kB/s]"
          }
        },
        "5c11bcb452884569b58f2c7b440cd528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d89f23c00a4e9cafc5b8a64ccec867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d13ca0b2b5b4b36a37f27ce8a1240c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1ed91e027a14541932353817d7ae22e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d666a7dfe24b7888b961cbf0377f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "727d39a44a7d4b6a9732063ef3404fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcd90e9c01484d8583657042caea8579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79bc37039bc14ad19ad7f15a47091d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfc1397a330b46acbcde8f18fd888169",
              "IPY_MODEL_43d0c55181e8491eb39f76e7f33d9bac",
              "IPY_MODEL_29490c6833804f39a5ff2107527a25dc"
            ],
            "layout": "IPY_MODEL_5e581bc74c7a42c3915053e85970be52"
          }
        },
        "dfc1397a330b46acbcde8f18fd888169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db183292437423789785690cef0b3ac",
            "placeholder": "​",
            "style": "IPY_MODEL_b108ca3c59f04af3a72736613643c255",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "43d0c55181e8491eb39f76e7f33d9bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427db90ad25d4dccabe700bd6992c68c",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93ee96f09dc44ac6b0df6f4787cbb015",
            "value": 440449768
          }
        },
        "29490c6833804f39a5ff2107527a25dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239a1ab9589f4da8861868bf7d727fb4",
            "placeholder": "​",
            "style": "IPY_MODEL_af50abf9ed944dfaa57daf35df957616",
            "value": " 440M/440M [00:02&lt;00:00, 141MB/s]"
          }
        },
        "5e581bc74c7a42c3915053e85970be52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3db183292437423789785690cef0b3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b108ca3c59f04af3a72736613643c255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "427db90ad25d4dccabe700bd6992c68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ee96f09dc44ac6b0df6f4787cbb015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "239a1ab9589f4da8861868bf7d727fb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af50abf9ed944dfaa57daf35df957616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbnc7mLTlUX9",
        "outputId": "bf9a3f1e-05bb-40cb-8a37-0f79deb0fcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE"
      ],
      "metadata": {
        "id": "ulvBC_3Cvmrr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ACYBGJtiyheA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Connect the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qdnb_tcnJB0",
        "outputId": "fdfd6776-3f41-45b5-ff7d-925668ef933b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymncWb-SApFw",
        "outputId": "c86f193c-30c6-40c8-a35b-8ac10e25fbc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data\n",
        "newswire = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/newswireFinal.csv')\n",
        "ceylon = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/ceylon_data.csv')\n",
        "hiru = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/hirunewsFinal.csv')\n",
        "island = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/island_data.csv')\n",
        "dailynews = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/dailynews.csv')\n",
        "lanka = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/lankanewswebFinal.csv')\n",
        "colombogazette = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/colomboGazetteFinal.csv')\n",
        "tamilguardian = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/tamilgurdianFinal.csv')\n",
        "adaderana = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/adaderana.csv')\n",
        "dailymirror = pd.read_csv('/content/drive/MyDrive/DataScraping/CSV/dailymirror.csv')"
      ],
      "metadata": {
        "id": "jMLUYr09mYVD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Source column\n",
        "newswire['Source']='newswire'\n",
        "ceylon['Source']='ceylon'\n",
        "hiru['Source']='hiru'\n",
        "island['Source']='island'\n",
        "dailynews['Source']='dailynews'\n",
        "lanka['Source']='lanka'\n",
        "colombogazette['Source']='colombogazette'\n",
        "tamilguardian['Source']='tamilguardian'\n",
        "adaderana['Source']='adaderana'\n",
        "dailymirror['Source']='dailymirror'"
      ],
      "metadata": {
        "id": "CpIkW8-smY0R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "def clean_text(text):\n",
        "    # Remove markup\n",
        "    text = re.sub(r'<.*?>', '', str(text))\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Clean the text data\n",
        "newswire['content'] = newswire['content'].apply(clean_text)\n",
        "ceylon['Content'] = ceylon['Content'].apply(clean_text)\n",
        "hiru['content'] = hiru['content'].apply(clean_text)\n",
        "island['Content'] = island['Content'].apply(clean_text)\n",
        "dailynews['content'] = dailynews['content'].apply(clean_text)\n",
        "lanka['content'] = lanka['content'].apply(clean_text)\n",
        "colombogazette['content'] = colombogazette['content'].apply(clean_text)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(clean_text)\n",
        "adaderana['Content'] = adaderana['Content'].apply(clean_text)\n",
        "dailymirror['content'] = dailymirror['content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "dJA9ddRxmY3k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and stop word removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def tokenize_remove_stopwords(text):\n",
        "    text = word_tokenize(text)\n",
        "    text = [token for token in text if token not in stop_words and token.isalpha()]\n",
        "    return text\n",
        "\n",
        "\n",
        "# Tokenize and remove stopwords from text data\n",
        "newswire['content'] = newswire['content'].apply(tokenize_remove_stopwords)\n",
        "ceylon['content'] = ceylon['Content'].apply(tokenize_remove_stopwords)\n",
        "hiru['content'] = hiru['content'].apply(tokenize_remove_stopwords)\n",
        "island['content'] = island['Content'].apply(tokenize_remove_stopwords)\n",
        "dailynews['content'] = dailynews['content'].apply(tokenize_remove_stopwords)\n",
        "lanka['content'] = lanka['content'].apply(tokenize_remove_stopwords)\n",
        "colombogazette['content'] = colombogazette['content'].apply(tokenize_remove_stopwords)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(tokenize_remove_stopwords)\n",
        "adaderana['content'] = adaderana['Content'].apply(tokenize_remove_stopwords)\n",
        "dailymirror['content'] = dailymirror['content'].apply(tokenize_remove_stopwords)"
      ],
      "metadata": {
        "id": "wf6hXBRKmZJ2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_token_counts(data):\n",
        "    # Combine all tokens into a single list\n",
        "    all_tokens = [token for tokens_list in data['content'] for token in tokens_list]\n",
        "\n",
        "    # Calculate the total number of tokens\n",
        "    total_tokens_count = len(all_tokens)\n",
        "\n",
        "    # Calculate the unique number of tokens using a set\n",
        "    unique_tokens_set = set(all_tokens)\n",
        "    unique_tokens_count = len(unique_tokens_set)\n",
        "\n",
        "    return total_tokens_count, unique_tokens_count\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(newswire)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(ceylon)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(hiru)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(island)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(dailynews)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(lanka)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(colombogazette)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(tamilguardian)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(adaderana)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(dailymirror)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvMXuipfSS3X",
        "outputId": "7b8db93e-f51c-4a29-b2b7-a800e0a7cd53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 844029\n",
            "Unique number of tokens: 34434\n",
            "Total number of tokens: 17673\n",
            "Unique number of tokens: 4224\n",
            "Total number of tokens: 426359\n",
            "Unique number of tokens: 27166\n",
            "Total number of tokens: 608694\n",
            "Unique number of tokens: 30845\n",
            "Total number of tokens: 358665\n",
            "Unique number of tokens: 22391\n",
            "Total number of tokens: 676331\n",
            "Unique number of tokens: 28494\n",
            "Total number of tokens: 376758\n",
            "Unique number of tokens: 20759\n",
            "Total number of tokens: 180095\n",
            "Unique number of tokens: 14834\n",
            "Total number of tokens: 1176890\n",
            "Unique number of tokens: 38051\n",
            "Total number of tokens: 980303\n",
            "Unique number of tokens: 35542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate total and unique token counts\n",
        "def calculate_token_counts(data):\n",
        "\n",
        "    # Calculate the total number of tokens\n",
        "    total_tokens_count = data['content'].apply(len).sum()\n",
        "\n",
        "    # Calculate the unique number of tokens\n",
        "    unique_tokens_set = set()\n",
        "    for tokens_list in data['content']:\n",
        "        unique_tokens_set.update(tokens_list)\n",
        "\n",
        "    unique_tokens_count = len(unique_tokens_set)\n",
        "\n",
        "    return total_tokens_count, unique_tokens_count\n",
        "\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(newswire)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(ceylon)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(hiru)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(island)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(dailynews)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(lanka)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(colombogazette)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(tamilguardian)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(adaderana)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)\n",
        "\n",
        "# Call the function with the sample data\n",
        "total_tokens, unique_tokens = calculate_token_counts(dailymirror)\n",
        "\n",
        "print(\"Total number of tokens:\", total_tokens)\n",
        "print(\"Unique number of tokens:\", unique_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkybApRjqO5v",
        "outputId": "ee134c01-a2c9-46c1-9819-59e0ff7beae0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 844029\n",
            "Unique number of tokens: 34434\n",
            "Total number of tokens: 17673\n",
            "Unique number of tokens: 4224\n",
            "Total number of tokens: 426359\n",
            "Unique number of tokens: 27166\n",
            "Total number of tokens: 608694\n",
            "Unique number of tokens: 30845\n",
            "Total number of tokens: 358665\n",
            "Unique number of tokens: 22391\n",
            "Total number of tokens: 676331\n",
            "Unique number of tokens: 28494\n",
            "Total number of tokens: 376758\n",
            "Unique number of tokens: 20759\n",
            "Total number of tokens: 180095\n",
            "Unique number of tokens: 14834\n",
            "Total number of tokens: 1176890\n",
            "Unique number of tokens: 38051\n",
            "Total number of tokens: 980303\n",
            "Unique number of tokens: 35542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "  stemmed_words = [stemmer.stem(word) for word in text]\n",
        "  return \" \".join(stemmed_words)\n",
        "\n",
        "# Apply stemming to text data\n",
        "newswire['content'] = newswire['content'].apply(stem_text)\n",
        "ceylon['content'] = ceylon['Content'].apply(stem_text)\n",
        "hiru['content'] = hiru['content'].apply(stem_text)\n",
        "island['content'] = island['Content'].apply(stem_text)\n",
        "dailynews['content'] = dailynews['content'].apply(stem_text)\n",
        "lanka['content'] = lanka['content'].apply(stem_text)\n",
        "colombogazette['content'] = colombogazette['content'].apply(stem_text)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(stem_text)\n",
        "adaderana['content'] = adaderana['Content'].apply(stem_text)\n",
        "dailymirror['content'] = dailymirror['content'].apply(stem_text)\n"
      ],
      "metadata": {
        "id": "YWpo9Tz-qO9b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources (only needs to be done once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('snowball_data')  # Snowball stemmer data\n",
        "\n",
        "# Initialize the Snowball Stemmer with the desired language\n",
        "stemmer = SnowballStemmer(language='english')  # Replace 'english' with the appropriate language if needed\n",
        "\n",
        "def snowball_stem_text(text):\n",
        "  stemmed_words = [stemmer.stem(word) for word in text]\n",
        "  return \" \".join(stemmed_words)\n",
        "\n",
        "# Apply stemming to text data\n",
        "newswire['content'] = newswire['content'].apply(snowball_stem_text)\n",
        "ceylon['content'] = ceylon['Content'].apply(snowball_stem_text)\n",
        "hiru['content'] = hiru['content'].apply(snowball_stem_text)\n",
        "island['content'] = island['Content'].apply(snowball_stem_text)\n",
        "dailynews['content'] = dailynews['content'].apply(snowball_stem_text)\n",
        "lanka['content'] = lanka['content'].apply(snowball_stem_text)\n",
        "colombogazette['content'] = colombogazette['content'].apply(snowball_stem_text)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(snowball_stem_text)\n",
        "adaderana['content'] = adaderana['Content'].apply(snowball_stem_text)\n",
        "dailymirror['content'] = dailymirror['content'].apply(snowball_stem_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTrfXKg1T9-",
        "outputId": "a77d7d14-6e3a-4194-e5a5-6009eeea8909"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package snowball_data to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download WordNet if not already downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in text]  # Use 'v' for verbs\n",
        "  return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply stemming to text data\n",
        "newswire['content'] = newswire['content'].apply(lemmatize_text)\n",
        "ceylon['content'] = ceylon['Content'].apply(lemmatize_text)\n",
        "hiru['content'] = hiru['content'].apply(lemmatize_text)\n",
        "island['content'] = island['Content'].apply(lemmatize_text)\n",
        "dailynews['content'] = dailynews['content'].apply(lemmatize_text)\n",
        "lanka['content'] = lanka['content'].apply(lemmatize_text)\n",
        "colombogazette['content'] = colombogazette['content'].apply(lemmatize_text)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(lemmatize_text)\n",
        "adaderana['content'] = adaderana['Content'].apply(lemmatize_text)\n",
        "dailymirror['content'] = dailymirror['content'].apply(lemmatize_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUvf7ubvyo-K",
        "outputId": "e4b8ecef-c345-4a72-c442-e543461302fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
        "  return \" \".join(lemmatized_words)\n",
        "\n",
        "# Apply lemmatization to text data\n",
        "newswire['content'] = newswire['content'].apply(lemmatize_text)\n",
        "ceylon['content'] = ceylon['Content'].apply(lemmatize_text)\n",
        "hiru['content'] = hiru['content'].apply(lemmatize_text)\n",
        "island['content'] = island['Content'].apply(lemmatize_text)\n",
        "dailynews['content'] = dailynews['content'].apply(lemmatize_text)\n",
        "lanka['content'] = lanka['content'].apply(lemmatize_text)\n",
        "colombogazette['content'] = colombogazette['content'].apply(lemmatize_text)\n",
        "tamilguardian['content'] = tamilguardian['content'].apply(lemmatize_text)\n",
        "adaderana['content'] = adaderana['Content'].apply(lemmatize_text)\n",
        "dailymirror['content'] = dailymirror['content'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "0YPtiYoTqO_B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Method1:Resampling\n",
        "#Class Weights\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Assuming you have the DataFrame 'articles_data' with columns 'content' and 'news_agency'\n",
        "data_frames = [newswire, ceylon, hiru, island, dailynews , lanka, colombogazette, tamilguardian,adaderana,dailymirror]\n",
        "articles_data = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Print class distribution before resampling\n",
        "print(\"Class Distribution before Resampling:\")\n",
        "print(articles_data['Source'].value_counts())\n",
        "\n",
        "# Step 1: Count the number of instances per news agency\n",
        "news_agency_counts = articles_data['Source'].value_counts()\n",
        "\n",
        "# Step 2: Calculate the class weights for each news agency\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=news_agency_counts.index, y=articles_data['Source'])\n",
        "class_weights_dict = dict(zip(news_agency_counts.index, class_weights))\n",
        "\n",
        "# Step 3: Apply resampling to balance the class distribution\n",
        "balanced_articles_data = pd.DataFrame()\n",
        "for news_agency in news_agency_counts.index:\n",
        "    df_subset = articles_data[articles_data['Source'] == news_agency]\n",
        "\n",
        "    # Choose either oversampling or undersampling based on the class weights\n",
        "    if class_weights_dict[news_agency] > 1.0:\n",
        "        # Oversample the minority class\n",
        "        df_subset_resampled = resample(df_subset, replace=True, n_samples=news_agency_counts.max(), random_state=42)\n",
        "    else:\n",
        "        # Undersample the majority class\n",
        "        df_subset_resampled = resample(df_subset, replace=False, n_samples=news_agency_counts.min(), random_state=42)\n",
        "\n",
        "    balanced_articles_data = pd.concat([balanced_articles_data, df_subset_resampled])\n",
        "\n",
        "\n",
        "# Now 'balanced_articles_data' contains the resampled dataset with balanced class distribution.\n",
        "# Print class distribution after resampling\n",
        "# Calculate the class distribution (number of instances per news agency)\n",
        "class_distribution = balanced_articles_data['Source'].value_counts()\n",
        "\n",
        "# Print the class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(class_distribution)\n",
        "\n",
        "# Calculate the percentage of instances for each news agency\n",
        "percentage_distribution = class_distribution / len(articles_data) * 100\n",
        "\n",
        "# Print the percentage distribution\n",
        "print(\"\\nPercentage Distribution:\")\n",
        "print(percentage_distribution)\n",
        "\n"
      ],
      "metadata": {
        "id": "N5HRwQ1AqPCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4114416e-55c2-4a68-d883-8519b99e9836"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution before Resampling:\n",
            "dailymirror       9029\n",
            "adaderana         8453\n",
            "newswire          6894\n",
            "hiru              4141\n",
            "island            3997\n",
            "lanka             3898\n",
            "colombogazette    2695\n",
            "dailynews         2500\n",
            "tamilguardian     1248\n",
            "ceylon             325\n",
            "Name: Source, dtype: int64\n",
            "Class Distribution:\n",
            "hiru              9029\n",
            "island            9029\n",
            "lanka             9029\n",
            "colombogazette    9029\n",
            "dailynews         9029\n",
            "tamilguardian     9029\n",
            "ceylon            9029\n",
            "dailymirror        325\n",
            "adaderana          325\n",
            "newswire           325\n",
            "Name: Source, dtype: int64\n",
            "\n",
            "Percentage Distribution:\n",
            "hiru              20.910144\n",
            "island            20.910144\n",
            "lanka             20.910144\n",
            "colombogazette    20.910144\n",
            "dailynews         20.910144\n",
            "tamilguardian     20.910144\n",
            "ceylon            20.910144\n",
            "dailymirror        0.752663\n",
            "adaderana          0.752663\n",
            "newswire           0.752663\n",
            "Name: Source, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Assuming you have the DataFrame 'balanced_articles_data' with columns 'content' and 'news_agency'\n",
        "\n",
        "# Step 1: Remove empty documents\n",
        "balanced_articles_data = balanced_articles_data[balanced_articles_data['content'].apply(lambda x: len(x.strip()) > 0)]\n",
        "\n",
        "# Step 2: Sparse Vector Representation using TF-IDF with custom tokenizer\n",
        "def custom_tokenizer(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english')\n",
        "\n",
        "# Fit and transform the text data to obtain sparse TF-IDF vectors\n",
        "sparse_tfidf_vectors = tfidf_vectorizer.fit_transform(balanced_articles_data['content'])\n",
        "\n",
        "# Step 3: Dense Vector Representation using Pre-trained Word Embeddings\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def get_average_word_embedding(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    word_embeddings = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
        "    if not word_embeddings:\n",
        "        return [0] * 300\n",
        "    return sum(word_embeddings) / len(word_embeddings)\n",
        "\n",
        "balanced_articles_data['word_embedding'] = balanced_articles_data['content'].apply(get_average_word_embedding)\n",
        "\n",
        "# Step 4: Split the data into training and testing sets for both representations\n",
        "X_train_sparse, X_test_sparse, y_train, y_test = train_test_split(sparse_tfidf_vectors, balanced_articles_data['Source'], test_size=0.2, random_state=42)\n",
        "X_train_dense, X_test_dense, _, _ = train_test_split(list(balanced_articles_data['word_embedding']), balanced_articles_data['Source'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train classifiers for both representations (e.g., Logistic Regression)\n",
        "sparse_classifier = LogisticRegression()\n",
        "sparse_classifier.fit(X_train_sparse, y_train)\n",
        "\n",
        "dense_classifier = LogisticRegression()\n",
        "dense_classifier.fit(X_train_dense, y_train)\n",
        "\n",
        "# Step 6: Make predictions and evaluate the classifiers\n",
        "y_pred_sparse = sparse_classifier.predict(X_test_sparse)\n",
        "y_pred_dense = dense_classifier.predict(X_test_dense)\n",
        "\n",
        "accuracy_sparse = accuracy_score(y_test, y_pred_sparse)\n",
        "accuracy_dense = accuracy_score(y_test, y_pred_dense)\n",
        "\n",
        "print(\"Accuracy using Sparse Vector (TF-IDF):\", accuracy_sparse)\n",
        "print(\"Accuracy using Dense Vector (Word Embeddings):\", accuracy_dense)\n",
        "\n"
      ],
      "metadata": {
        "id": "PHLu6hpXqPNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6004b005-891e-4c43-bac2-88abc6a62bfe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 1661.3/1662.8MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Sparse Vector (TF-IDF): 0.837542662116041\n",
            "Accuracy using Dense Vector (Word Embeddings): 0.7868031854379978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Train classifiers using the sparse vector representation (TF-IDF)\n",
        "logistic_regression_sparse = LogisticRegression(max_iter=100,solver='saga')\n",
        "logistic_regression_sparse.fit(X_train_sparse, y_train)\n",
        "\n",
        "random_forest_sparse = RandomForestClassifier()\n",
        "random_forest_sparse.fit(X_train_sparse, y_train)\n",
        "\n",
        "svm_sparse = SVC()\n",
        "svm_sparse.fit(X_train_sparse, y_train)\n",
        "\n",
        "# Train classifiers using the dense vector representation (Word2Vec)\n",
        "logistic_regression_dense = LogisticRegression(max_iter=100,solver='saga')\n",
        "logistic_regression_dense.fit(X_train_dense, y_train)\n",
        "\n",
        "random_forest_dense = RandomForestClassifier()\n",
        "random_forest_dense.fit(X_train_dense, y_train)\n",
        "\n",
        "svm_dense = SVC()\n",
        "svm_dense.fit(X_train_dense, y_train)\n",
        "\n",
        "# Evaluate the classifiers on the test set\n",
        "accuracy_sparse_logreg = accuracy_score(y_test, logistic_regression_sparse.predict(X_test_sparse))\n",
        "accuracy_sparse_rf = accuracy_score(y_test, random_forest_sparse.predict(X_test_sparse))\n",
        "accuracy_sparse_svm = accuracy_score(y_test, svm_sparse.predict(X_test_sparse))\n",
        "\n",
        "# Evaluate classifiers on the test set using sparse representation (TF-IDF)\n",
        "y_pred_logreg_sparse = logistic_regression_sparse.predict(X_test_sparse)\n",
        "y_pred_rf_sparse = random_forest_sparse.predict(X_test_sparse)\n",
        "y_pred_svm_sparse = svm_sparse.predict(X_test_sparse)\n",
        "\n",
        "precision_logreg_sparse = precision_score(y_test, y_pred_logreg_sparse, average='weighted',zero_division=1)\n",
        "precision_rf_sparse = precision_score(y_test, y_pred_rf_sparse, average='weighted',zero_division=1)\n",
        "precision_svm_sparse = precision_score(y_test, y_pred_svm_sparse, average='weighted',zero_division=1)\n",
        "\n",
        "recall_logreg_sparse = recall_score(y_test, y_pred_logreg_sparse, average='weighted',zero_division=1)\n",
        "recall_rf_sparse = recall_score(y_test, y_pred_rf_sparse, average='weighted',zero_division=1)\n",
        "recall_svm_sparse = recall_score(y_test, y_pred_svm_sparse, average='weighted',zero_division=1)\n",
        "\n",
        "f1_logreg_sparse = f1_score(y_test, y_pred_logreg_sparse, average='weighted',zero_division=1)\n",
        "f1_rf_sparse = f1_score(y_test, y_pred_rf_sparse, average='weighted',zero_division=1)\n",
        "f1_svm_sparse = f1_score(y_test, y_pred_svm_sparse, average='weighted',zero_division=1)\n",
        "\n",
        "confusion_matrix_logreg_sparse = confusion_matrix(y_test, y_pred_logreg_sparse)\n",
        "confusion_matrix_rf_sparse = confusion_matrix(y_test, y_pred_rf_sparse)\n",
        "confusion_matrix_svm_sparse = confusion_matrix(y_test, y_pred_svm_sparse)\n",
        "\n",
        "#Dense\n",
        "accuracy_dense_logreg = accuracy_score(y_test, logistic_regression_dense.predict(X_test_dense))\n",
        "accuracy_dense_rf = accuracy_score(y_test, random_forest_dense.predict(X_test_dense))\n",
        "accuracy_dense_svm = accuracy_score(y_test, svm_dense.predict(X_test_dense))\n",
        "\n",
        "y_pred_logreg_dense = logistic_regression_dense.predict(X_test_dense)\n",
        "y_pred_rf_dense = random_forest_dense.predict(X_test_dense)\n",
        "y_pred_svm_dense = svm_dense.predict(X_test_dense)\n",
        "\n",
        "precision_logreg_dense = precision_score(y_test, y_pred_logreg_dense, average='weighted',zero_division=1)\n",
        "precision_rf_dense = precision_score(y_test, y_pred_rf_dense, average='weighted',zero_division=1)\n",
        "precision_svm_dense = precision_score(y_test, y_pred_svm_dense, average='weighted',zero_division=1)\n",
        "\n",
        "recall_logreg_dense = recall_score(y_test, y_pred_logreg_dense, average='weighted',zero_division=1)\n",
        "recall_rf_dense = recall_score(y_test, y_pred_rf_dense, average='weighted',zero_division=1)\n",
        "recall_svm_dense = recall_score(y_test, y_pred_svm_dense, average='weighted',zero_division=1)\n",
        "\n",
        "f1_logreg_dense = f1_score(y_test, y_pred_logreg_dense, average='weighted',zero_division=1)\n",
        "f1_rf_dense = f1_score(y_test, y_pred_rf_dense, average='weighted',zero_division=1)\n",
        "f1_svm_dense = f1_score(y_test, y_pred_svm_dense, average='weighted',zero_division=1)\n",
        "\n",
        "confusion_matrix_logreg_dense = confusion_matrix(y_test, y_pred_logreg_dense)\n",
        "confusion_matrix_rf_dense = confusion_matrix(y_test, y_pred_rf_dense)\n",
        "confusion_matrix_svm_dense = confusion_matrix(y_test, y_pred_svm_dense)\n",
        "\n",
        "print(\"Sparse Vector - TF-IDF\")\n",
        "print(\"Logistic Regression - Accuracy:\", accuracy_sparse_logreg)\n",
        "print(\"Logistic Regression - Precision:\", precision_logreg_sparse)\n",
        "print(\"Logistic Regression - Recall:\", recall_logreg_sparse)\n",
        "print(\"Logistic Regression - F1-Score:\", f1_logreg_sparse)\n",
        "print(\"Logistic Regression - Confusion Matrix:\\n\", confusion_matrix_logreg_sparse)\n",
        "\n",
        "print(\"\\nRandom Forest - Accuracy:\", accuracy_sparse_rf)\n",
        "print(\"Random Forest - Precision:\", precision_rf_sparse)\n",
        "print(\"Random Forest - Recall:\", recall_rf_sparse)\n",
        "print(\"Random Forest - F1-Score:\", f1_rf_sparse)\n",
        "print(\"Random Forest - Confusion Matrix:\\n\", confusion_matrix_rf_sparse)\n",
        "\n",
        "print(\"\\nSVM - Accuracy:\", accuracy_sparse_svm)\n",
        "print(\"SVM - Precision:\", precision_svm_sparse)\n",
        "print(\"SVM - Recall:\", recall_svm_sparse)\n",
        "print(\"SVM - F1-Score:\", f1_svm_sparse)\n",
        "print(\"SVM - Confusion Matrix:\\n\", confusion_matrix_svm_sparse)\n",
        "\n",
        "print(\"\\nDense Vector - Word2Vec Embeddings\")\n",
        "print(\"Logistic Regression - Accuracy:\", accuracy_dense_logreg)\n",
        "print(\"Logistic Regression - Precision:\", precision_logreg_dense)\n",
        "print(\"Logistic Regression - Recall:\", recall_logreg_dense)\n",
        "print(\"Logistic Regression - F1-Score:\", f1_logreg_dense)\n",
        "print(\"Logistic Regression - Confusion Matrix:\\n\", confusion_matrix_logreg_dense)\n",
        "\n",
        "print(\"\\nRandom Forest - Accuracy:\", accuracy_dense_rf)\n",
        "print(\"Random Forest - Precision:\", precision_rf_dense)\n",
        "print(\"Random Forest - Recall:\", recall_rf_dense)\n",
        "print(\"Random Forest - F1-Score:\", f1_rf_dense)\n",
        "print(\"Random Forest - Confusion Matrix:\\n\", confusion_matrix_rf_dense)\n",
        "\n",
        "print(\"\\nSVM - Accuracy:\", accuracy_dense_svm)\n",
        "print(\"SVM - Precision:\", precision_svm_dense)\n",
        "print(\"SVM - Recall:\", recall_svm_dense)\n",
        "print(\"SVM - F1-Score:\", f1_svm_dense)\n",
        "print(\"SVM - Confusion Matrix:\\n\", confusion_matrix_svm_dense)\n",
        "\n"
      ],
      "metadata": {
        "id": "5fB_7ptGqPQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c58649-fad2-4702-8983-ae2ff7e7ed9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse Vector - TF-IDF\n",
            "Logistic Regression - Accuracy: 0.837542662116041\n",
            "Logistic Regression - Precision: 0.8472041521126611\n",
            "Logistic Regression - Recall: 0.837542662116041\n",
            "Logistic Regression - F1-Score: 0.8153799847731121\n",
            "Logistic Regression - Confusion Matrix:\n",
            " [[1407    0    0    0    0    0    2]\n",
            " [   2 1180    0    0    0    0  232]\n",
            " [   1   51    0    0    0    0   14]\n",
            " [  57    0    0    0    0    0    0]\n",
            " [   0   40    0    0    0    0   16]\n",
            " [   0   32    0    0    0    0   20]\n",
            " [   0  247    0    0    0    0 1094]]\n",
            "\n",
            "Random Forest - Accuracy: 0.9569965870307168\n",
            "Random Forest - Precision: 0.9411249907602085\n",
            "Random Forest - Recall: 0.9569965870307168\n",
            "Random Forest - F1-Score: 0.9416676193768047\n",
            "Random Forest - Confusion Matrix:\n",
            " [[1409    0    0    0    0    0    0]\n",
            " [   0 1405    0    0    1    0    8]\n",
            " [   0   35    3    0    5    4   19]\n",
            " [  11    0    0   46    0    0    0]\n",
            " [   0   32    0    0    1    1   22]\n",
            " [   0   20    1    0    1    5   25]\n",
            " [   0    4    0    0    0    0 1337]]\n",
            "\n",
            "SVM - Accuracy: 0.8641638225255973\n",
            "SVM - Precision: 0.8723420009961659\n",
            "SVM - Recall: 0.8641638225255973\n",
            "SVM - F1-Score: 0.8412437852077673\n",
            "SVM - Confusion Matrix:\n",
            " [[1409    0    0    0    0    0    0]\n",
            " [   0 1234    0    0    0    0  180]\n",
            " [   0   54    0    0    0    0   12]\n",
            " [  57    0    0    0    0    0    0]\n",
            " [   0   40    0    0    0    0   16]\n",
            " [   0   34    0    0    0    0   18]\n",
            " [   0  186    0    0    0    0 1155]]\n",
            "\n",
            "Dense Vector - Word2Vec Embeddings\n",
            "Logistic Regression - Accuracy: 0.7899886234357224\n",
            "Logistic Regression - Precision: 0.801697011733211\n",
            "Logistic Regression - Recall: 0.7899886234357224\n",
            "Logistic Regression - F1-Score: 0.7688178689463159\n",
            "Logistic Regression - Confusion Matrix:\n",
            " [[1389    6    0    0    0    0   14]\n",
            " [  32 1084    0    0    0    0  298]\n",
            " [   2   49    0    0    0    0   15]\n",
            " [  55    1    0    0    0    0    1]\n",
            " [   0   45    0    0    0    0   11]\n",
            " [   0   34    0    0    0    0   18]\n",
            " [   6  336    0    0    0    0  999]]\n",
            "\n",
            "Random Forest - Accuracy: 0.9465301478953356\n",
            "Random Forest - Precision: 0.9310819579172147\n",
            "Random Forest - Recall: 0.9465301478953356\n",
            "Random Forest - F1-Score: 0.9269720646831204\n",
            "Random Forest - Confusion Matrix:\n",
            " [[1409    0    0    0    0    0    0]\n",
            " [   0 1400    1    0    1    0   12]\n",
            " [   0   63    1    1    0    0    1]\n",
            " [  29    8    0   20    0    0    0]\n",
            " [   0   54    0    0    0    0    2]\n",
            " [   0   49    0    0    0    1    2]\n",
            " [   0   12    0    0    0    0 1329]]\n",
            "\n",
            "SVM - Accuracy: 0.7943117178612059\n",
            "SVM - Precision: 0.8098657731441882\n",
            "SVM - Recall: 0.7943117178612059\n",
            "SVM - F1-Score: 0.7731490113628909\n",
            "SVM - Confusion Matrix:\n",
            " [[1387   14    0    0    0    0    8]\n",
            " [  21 1157    0    0    0    0  236]\n",
            " [   3   52    0    0    0    0   11]\n",
            " [  55    1    0    0    0    0    1]\n",
            " [   0   49    0    0    0    0    7]\n",
            " [   0   39    0    0    0    0   13]\n",
            " [   6  388    0    0    0    0  947]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRYeVp0NmEK",
        "outputId": "641d78bc-2639-45ab-ecca-954eeae9e5b9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Step 2: Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "balanced_articles_data['label'] = label_encoder.fit_transform(balanced_articles_data['Source'])\n",
        "\n",
        "# Step 3: Tokenize and pad the text data for deep learning models\n",
        "max_len = 100  # Maximum sequence length for padding\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(balanced_articles_data['content'])\n",
        "X_sequences = tokenizer.texts_to_sequences(balanced_articles_data['content'])\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
        "\n",
        "# Step 4: Load pre-trained Word2Vec embeddings\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Step 5: Load GloVe embeddings\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "# Step 6: Build deep learning models with different architectures\n",
        "\n",
        "# Create a custom embedding matrix based on the words available in the word2vec_model\n",
        "word_index = tokenizer.word_index\n",
        "embedding_dim = 300\n",
        "num_words = min(len(word_index) + 1, len(word2vec_model.key_to_index))\n",
        "embedding_matrix_w2v = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix_w2v[i] = word2vec_model[word]\n",
        "\n",
        "# Create a custom embedding matrix based on the words available in the glove_vectors\n",
        "embedding_matrix_glove = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    if word in glove_vectors:\n",
        "        embedding_matrix_glove[i] = glove_vectors[word]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLILkdagMunc",
        "outputId": "f004883b-ab74-4cb2-934c-067c77b7f6a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Model 1: LSTM with Word2Vec Embeddings\n",
        "model_lstm_w2v = Sequential()\n",
        "model_lstm_w2v.add(Embedding(input_dim=num_words, output_dim=embedding_dim, weights=[embedding_matrix_w2v], input_length=max_len, trainable=False))\n",
        "model_lstm_w2v.add(LSTM(64))\n",
        "model_lstm_w2v.add(Dropout(0.5))\n",
        "model_lstm_w2v.add(Dense(32, activation='relu'))\n",
        "model_lstm_w2v.add(Dropout(0.5))\n",
        "model_lstm_w2v.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "model_lstm_w2v.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 7: Train the deep learning models\n",
        "model_lstm_w2v.fit(X_padded, balanced_articles_data['label'], epochs=6, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 8: Evaluate the models on the test set\n",
        "y_prob_lstm_w2v = model_lstm_w2v.predict(X_padded)\n",
        "y_pred_lstm_w2v = np.argmax(y_prob_lstm_w2v, axis=1)\n",
        "accuracy_lstm_w2v = np.mean(y_pred_lstm_w2v == balanced_articles_data['label'])\n",
        "\n",
        "precision_lstm_w2v = precision_score(balanced_articles_data['label'], y_pred_lstm_w2v, average='weighted', zero_division=0)\n",
        "recall_lstm_w2v = recall_score(balanced_articles_data['label'], y_pred_lstm_w2v, average='weighted', zero_division=0)\n",
        "f1_lstm_w2v = f1_score(balanced_articles_data['label'], y_pred_lstm_w2v, average='weighted', zero_division=0)\n",
        "confusion_matrix_lstm_w2v = confusion_matrix(balanced_articles_data['label'], y_pred_lstm_w2v)\n",
        "\n",
        "print(\"LSTM with Word2Vec Embeddings Accuracy:\", accuracy_lstm_w2v)\n",
        "print(\"LSTM with Word2Vec Embeddings Precision:\", precision_lstm_w2v)\n",
        "print(\"LSTM with Word2Vec Embeddings Recall:\", recall_lstm_w2v)\n",
        "print(\"LSTM with Word2Vec Embeddings F1-Score:\", f1_lstm_w2v)\n",
        "print(\"LSTM with Word2Vec Embeddings Confusion Matrix:\\n\", confusion_matrix_lstm_w2v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhOZroigcFMg",
        "outputId": "01b6d101-042e-4ec5-b4db-517d22ccc402"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "550/550 [==============================] - 23s 20ms/step - loss: 0.9011 - accuracy: 0.7116 - val_loss: 0.9594 - val_accuracy: 0.6994\n",
            "Epoch 2/6\n",
            "550/550 [==============================] - 8s 14ms/step - loss: 0.6376 - accuracy: 0.8142 - val_loss: 0.3465 - val_accuracy: 0.9251\n",
            "Epoch 3/6\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 0.4999 - accuracy: 0.8619 - val_loss: 0.2747 - val_accuracy: 0.9377\n",
            "Epoch 4/6\n",
            "550/550 [==============================] - 7s 13ms/step - loss: 0.4365 - accuracy: 0.8806 - val_loss: 0.1689 - val_accuracy: 0.9768\n",
            "Epoch 5/6\n",
            "550/550 [==============================] - 8s 15ms/step - loss: 0.3909 - accuracy: 0.8906 - val_loss: 0.1043 - val_accuracy: 0.9916\n",
            "Epoch 6/6\n",
            "550/550 [==============================] - 4s 8ms/step - loss: 0.3636 - accuracy: 0.8968 - val_loss: 0.1265 - val_accuracy: 0.9961\n",
            "687/687 [==============================] - 3s 3ms/step\n",
            "LSTM with Word2Vec Embeddings Accuracy: 0.9289973146420282\n",
            "LSTM with Word2Vec Embeddings Precision: 0.8915221861188684\n",
            "LSTM with Word2Vec Embeddings Recall: 0.9289973146420282\n",
            "LSTM with Word2Vec Embeddings F1-Score: 0.9080288135730914\n",
            "LSTM with Word2Vec Embeddings Confusion Matrix:\n",
            " [[6871    0    0    0    0    0   23]\n",
            " [  12 6502    0    0    0    4  376]\n",
            " [  10   26    0    0    0    0  289]\n",
            " [ 293    6    0    0    0    0   26]\n",
            " [  10   23    0    0    0    0  292]\n",
            " [   5    9    0    0    0  194  106]\n",
            " [  13   25    0    0    0   12 6844]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: LSTM with GloVe Embeddings\n",
        "model_lstm_glove = Sequential()\n",
        "model_lstm_glove.add(Embedding(input_dim=num_words, output_dim=embedding_dim, weights=[embedding_matrix_glove], input_length=max_len, trainable=False))\n",
        "model_lstm_glove.add(LSTM(32))\n",
        "model_lstm_glove.add(Dropout(0.5))\n",
        "model_lstm_glove.add(Dense(16, activation='relu'))\n",
        "model_lstm_glove.add(Dropout(0.5))\n",
        "model_lstm_glove.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "model_lstm_glove.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_lstm_glove.fit(X_padded, balanced_articles_data['label'], epochs=6, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the LSTM model with GloVe embeddings on the test set\n",
        "y_prob_lstm_glove = model_lstm_glove.predict(X_padded)\n",
        "y_pred_lstm_glove = np.argmax(y_prob_lstm_glove, axis=1)\n",
        "accuracy_lstm_glove = np.mean(y_pred_lstm_glove == balanced_articles_data['label'])\n",
        "\n",
        "# Calculate precision, recall, and F1-score for each class\n",
        "precision_lstm_glove = precision_score(balanced_articles_data['label'], y_pred_lstm_glove, average='weighted',zero_division=0)\n",
        "recall_lstm_glove = recall_score(balanced_articles_data['label'], y_pred_lstm_glove, average='weighted',zero_division=0)\n",
        "f1_lstm_glove = f1_score(balanced_articles_data['label'], y_pred_lstm_glove, average='weighted',zero_division=0)\n",
        "# Calculate the confusion matrix\n",
        "confusion_matrix_lstm_glove = confusion_matrix(balanced_articles_data['label'], y_pred_lstm_glove)\n",
        "\n",
        "print(\"LSTM with GloVe Embeddings Accuracy:\", accuracy_lstm_glove)\n",
        "print(\"LSTM with GloVe Embeddings Precision:\", precision_lstm_glove)\n",
        "print(\"LSTM with GloVe Embeddings Recall:\", recall_lstm_glove)\n",
        "print(\"LSTM with GloVe Embeddings F1-Score:\", f1_lstm_glove)\n",
        "print(\"LSTM with GloVe Embeddings Confusion Matrix:\\n\", confusion_matrix_lstm_glove)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdEEbIo5aE3N",
        "outputId": "5851400d-05ca-4fef-8a43-aa85eefa030c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "138/138 [==============================] - 5s 12ms/step - loss: 1.1795 - accuracy: 0.5786 - val_loss: 1.4263 - val_accuracy: 0.0348\n",
            "Epoch 2/6\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.8543 - accuracy: 0.7451 - val_loss: 0.7635 - val_accuracy: 0.8066\n",
            "Epoch 3/6\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.7146 - accuracy: 0.8026 - val_loss: 0.2682 - val_accuracy: 0.9456\n",
            "Epoch 4/6\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.6435 - accuracy: 0.8229 - val_loss: 0.2354 - val_accuracy: 0.9618\n",
            "Epoch 5/6\n",
            "138/138 [==============================] - 1s 8ms/step - loss: 0.5958 - accuracy: 0.8347 - val_loss: 0.3373 - val_accuracy: 0.9413\n",
            "Epoch 6/6\n",
            "138/138 [==============================] - 2s 11ms/step - loss: 0.5615 - accuracy: 0.8400 - val_loss: 0.3653 - val_accuracy: 0.9265\n",
            "687/687 [==============================] - 3s 4ms/step\n",
            "LSTM with GloVe Embeddings Accuracy: 0.8767466205452642\n",
            "LSTM with GloVe Embeddings Precision: 0.8426968268772437\n",
            "LSTM with GloVe Embeddings Recall: 0.8767466205452642\n",
            "LSTM with GloVe Embeddings F1-Score: 0.8531798563693451\n",
            "LSTM with GloVe Embeddings Confusion Matrix:\n",
            " [[6407    0    0    0    0    0  487]\n",
            " [   8 5983    0    0    0    0  903]\n",
            " [   0    0    0    0    0    0  325]\n",
            " [ 237    1    0    0    0    0   87]\n",
            " [   0    3    0    0    0    0  322]\n",
            " [   0  143    0    0    0    0  171]\n",
            " [   9   12    0    0    0    0 6873]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "data_frames = [newswire, ceylon, hiru, island, lanka, colombogazette, tamilguardian]\n",
        "\n",
        "# Concatenate the data frames\n",
        "articles_data = pd.concat(data_frames, ignore_index=True)\n",
        "\n",
        "# Separate the features (X) and target (Y)\n",
        "X = articles_data['content'].apply(lambda x: ' '.join(x))\n",
        "Y = articles_data['Source']\n",
        "\n",
        "# Tokenize the text data\n",
        "def tokenize_text(X, tokenizer, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in X:\n",
        "        # Tokenize the text and add special tokens for BERT\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Set the maximum sequence length for BERT\n",
        "max_seq_length = 128\n",
        "\n",
        "# Tokenize the content data and get input IDs and attention masks\n",
        "X_input_ids, X_attention_masks = tokenize_text(X, tokenizer, max_seq_length)\n",
        "\n",
        "\n",
        "# Step 2: Encode the target labels\n",
        "label_encoder = LabelEncoder()\n",
        "articles_data['label'] = label_encoder.fit_transform(Y)\n",
        "\n",
        "def create_lstm_model():\n",
        "    input_ids = Input(shape=(max_seq_length,), dtype='int32')\n",
        "    attention_masks = Input(shape=(max_seq_length,), dtype='int32')\n",
        "\n",
        "    # Get the BERT embeddings for the input IDs\n",
        "    bert_output = bert_model(input_ids, attention_mask=attention_masks)[0]\n",
        "\n",
        "    # Use LSTM to process the BERT embeddings\n",
        "    lstm_output = LSTM(units=64)(bert_output)\n",
        "\n",
        "    # Add a dense layer for classification\n",
        "    output = Dense(len(label_encoder.classes_), activation='softmax')(lstm_output)\n",
        "\n",
        "    model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the LSTM model\n",
        "model = create_lstm_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    x=[X_input_ids, X_attention_masks],\n",
        "    y=articles_data['label'],\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# After training the model, you can calculate the predicted probabilities for the test set\n",
        "y_pred_prob_lstm = model.predict(x=[X_input_ids, X_attention_masks])\n",
        "\n",
        "# Get the predicted class label (index with the highest probability) for each sample\n",
        "y_pred_lstm = np.argmax(y_pred_prob_lstm, axis=1)\n",
        "\n",
        "# Calculate the evaluation metrics\n",
        "accuracy_lstm = accuracy_score(articles_data['label'], y_pred_lstm)\n",
        "print(\"LSTM Model Accuracy:\", accuracy_lstm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540,
          "referenced_widgets": [
            "0b70b9fd007c4612b5b88f51add052db",
            "bbff94df6a8a49a9961544d81ab26afe",
            "70ff9c76094c40538484d0e957791156",
            "336d31e11ca344e780e582c63e7e54fb",
            "111fd8f8dd4d4e909a347785b43d7c30",
            "5c2778836b924cb5a1040a7a21919c44",
            "a2a29b7221bb4916a680cd84778710fa",
            "3018ce91800d4d819f69ef7470e19f5e",
            "e1ad9980b3d347d3b2deb93c2bb5025e",
            "067b4b776ccd4a009dd4b4139758fa61",
            "5da90896f96e413fb9ddbc6bb4ee415d",
            "6965aeef611e487890e26bce845bdc88",
            "2907e544ff834df3b46f02e2a62fd900",
            "dadf1eda4823436582dffd14d9804d89",
            "9bac4ec7165541dab9105f22e4a33380",
            "bf525a57d6674f119dac5f323aac1c7c",
            "28218aea344e4a0bb1c2bb38e3fd6905",
            "95fd47ef822b4d0e9f3bba27f5eb83e6",
            "08ea208adacc4c68a86232b207adada6",
            "a7f903cd45dc4725a6d1414dfba0da34",
            "38f1fd5ab5494119935c7c408c587f45",
            "d97457bced2e412fbd09726ee3415edd",
            "8549db17b78f40338f907ca920970b29",
            "3002b80b699b435597644794472cafaa",
            "70086406573448029fc7e9e31b944b2c",
            "42bad065754548deafde71346a3616f6",
            "5c11bcb452884569b58f2c7b440cd528",
            "e4d89f23c00a4e9cafc5b8a64ccec867",
            "1d13ca0b2b5b4b36a37f27ce8a1240c8",
            "a1ed91e027a14541932353817d7ae22e",
            "c2d666a7dfe24b7888b961cbf0377f1c",
            "727d39a44a7d4b6a9732063ef3404fb4",
            "fcd90e9c01484d8583657042caea8579",
            "79bc37039bc14ad19ad7f15a47091d73",
            "dfc1397a330b46acbcde8f18fd888169",
            "43d0c55181e8491eb39f76e7f33d9bac",
            "29490c6833804f39a5ff2107527a25dc",
            "5e581bc74c7a42c3915053e85970be52",
            "3db183292437423789785690cef0b3ac",
            "b108ca3c59f04af3a72736613643c255",
            "427db90ad25d4dccabe700bd6992c68c",
            "93ee96f09dc44ac6b0df6f4787cbb015",
            "239a1ab9589f4da8861868bf7d727fb4",
            "af50abf9ed944dfaa57daf35df957616"
          ]
        },
        "id": "Ly62Z7h3GAMN",
        "outputId": "1b1602ff-a6ac-4a4e-fc86-0f734d754194"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b70b9fd007c4612b5b88f51add052db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6965aeef611e487890e26bce845bdc88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8549db17b78f40338f907ca920970b29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79bc37039bc14ad19ad7f15a47091d73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "580/580 [==============================] - 608s 956ms/step - loss: 1.4176 - accuracy: 0.3683 - val_loss: 7.8508 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/5\n",
            "580/580 [==============================] - 552s 952ms/step - loss: 1.4102 - accuracy: 0.3715 - val_loss: 8.7679 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "580/580 [==============================] - 552s 952ms/step - loss: 1.4096 - accuracy: 0.3715 - val_loss: 9.3325 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "580/580 [==============================] - 553s 953ms/step - loss: 1.4098 - accuracy: 0.3715 - val_loss: 9.7913 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "580/580 [==============================] - 552s 953ms/step - loss: 1.4097 - accuracy: 0.3715 - val_loss: 10.1742 - val_accuracy: 0.0000e+00\n",
            "725/725 [==============================] - 209s 284ms/step\n",
            "LSTM Model Accuracy: 0.29718079144753856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scraping**"
      ],
      "metadata": {
        "id": "WbvXfAnQsDED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "from datetime import datetime,timedelta\n",
        "import pandas as pd\n",
        "import pytz\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "ohFW2srWsnHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ceylontoday news Scraping\n",
        "# Create an empty list to store the news data\n",
        "news_data = []\n",
        "\n",
        "def scrape_individual_news(news_url):\n",
        "    individual_news = requests.get(news_url).text\n",
        "    soup2 = BeautifulSoup(individual_news, 'lxml')\n",
        "    dateString = soup2.find('time', class_='entry-date updated td-module-date')['datetime']\n",
        "    datetime_obj = datetime.strptime(dateString, '%Y-%m-%dT%H:%M:%S%z')\n",
        "    news_date = datetime_obj.strftime('%Y-%m-%d')\n",
        "    news_content_element = soup2.find('div', class_='td_block_wrap tdb_single_content tdi_108 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
        "\n",
        "    if news_content_element:\n",
        "        news_content = news_content_element.get_text(strip=True)\n",
        "        return news_date, news_content\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "for page in range(1, 351):\n",
        "    news_website_url = f\"https://ceylontoday.lk/category/local/page/{page}/\"\n",
        "    html_text = requests.get(news_website_url).text\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    reports = soup.find_all('div', class_='tdb_module_loop td_module_wrap td-animation-stack')\n",
        "\n",
        "    for report in reports:\n",
        "        news_title = report.find('div', class_='td-module-thumb').a['title']\n",
        "        news_url = report.find('div', class_='td-module-thumb').a['href']\n",
        "        news_date, news_content = scrape_individual_news(news_url)\n",
        "\n",
        "        if news_date and news_content:\n",
        "            news_data.append({\n",
        "                'Title': news_title,\n",
        "                'Date': news_date,\n",
        "                'URL': news_url,\n",
        "                'Content': news_content\n",
        "            })\n",
        "        else:\n",
        "            print(\"No content found.\")\n",
        "\n",
        "        # Introduce a short delay between requests to avoid overloading the server\n",
        "        time.sleep(1)\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'ceylon_data.csv'\n",
        "\n",
        "# Write the news data to the CSV file\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Title', 'Date', 'URL', 'Content']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write each news item as a row\n",
        "    for news_item in news_data:\n",
        "        writer.writerow(news_item)\n",
        "\n",
        "print(\"CSV file has been created successfully.\")"
      ],
      "metadata": {
        "id": "BJWr1gAEsHMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "date_format = \"%B %d, %Y\"\n",
        "target_date_str = \"July 10, 2022\"\n",
        "target_date = datetime.strptime(target_date_str, date_format)\n",
        "reached = False\n",
        "\n",
        "for i in range(1, 300):\n",
        "    print(f\"Page Number: {i}\")\n",
        "    url = f\"https://colombogazette.com/category/news/page/{i}/\"\n",
        "    response = requests.get(url).content\n",
        "    bs = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "    listOfNews = bs.find('div', class_=\"td-ss-main-content\")\n",
        "    articles = listOfNews.find_all('div', class_=\"td-block-row\")\n",
        "\n",
        "    for article in articles:\n",
        "        newsContainer = article.find_all('div', class_=\"td-block-span6\")\n",
        "        for data in newsContainer:\n",
        "            title = data.find('h3').text.strip()\n",
        "            contentURL = data.find('h3').find('a')['href']\n",
        "            dateString = data.find('div', class_=\"td-module-meta-info\").find('time').text.strip()\n",
        "            formatedDate = datetime.strptime(dateString, date_format)\n",
        "\n",
        "            if target_date <= formatedDate:\n",
        "                print(formatedDate)\n",
        "                moreInfo = requests.get(contentURL).content\n",
        "                bs2 = BeautifulSoup(moreInfo, 'lxml')\n",
        "                contentContainer = bs2.find('div', class_=\"td-theme-wrap\").find('article').find('div', class_=\"td-post-content tagdiv-type\").find_all('p')\n",
        "                tempContent = [info.text.strip() for info in contentContainer]\n",
        "                news.append([title, formatedDate.strftime(date_format), \" \".join(tempContent)])\n",
        "                tempContent = []\n",
        "            else:\n",
        "                reached = True\n",
        "                break\n",
        "\n",
        "        print(f\"Total News as of now: {len(news)}\")\n",
        "        if reached:\n",
        "            print(\"Reached the given date limit.\")\n",
        "            break\n",
        "\n",
        "    if reached:\n",
        "        break  # No need to continue looping once target date is reached\n",
        "\n",
        "print(\"Scraping completed.\")\n",
        "\n",
        "# Now you can process the 'news' list as needed, such as writing it to a CSV file."
      ],
      "metadata": {
        "id": "omM9h9EvsUPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "date_format = \"%A, %d %B %Y - %H:%M\"\n",
        "target_date_str = \"Monday, 18 July 2022 - 00:00\"\n",
        "target_date = datetime.strptime(target_date_str, date_format)\n",
        "reached = False\n",
        "\n",
        "print(target_date)\n",
        "\n",
        "for i in range(1, 1500):\n",
        "    print(f\"Page Number: {i}\")\n",
        "    url = f\"https://www.hirunews.lk/english/local-news.php?pageID={i}\"\n",
        "    response = requests.get(url).content\n",
        "    bs = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "    listOfNews = bs.find('div', class_=\"trending-section\")\n",
        "    articles = listOfNews.find_all('div', class_=\"row\")\n",
        "\n",
        "    for article in articles:\n",
        "        title = article.find('div', class_=\"column middle\").find(\"a\").text\n",
        "        dateString = article.find('div', class_=\"middle-tittle-time\").text.strip()\n",
        "        formatedDate = datetime.strptime(dateString, date_format)\n",
        "\n",
        "        if target_date < formatedDate:\n",
        "            contentURL = article.find('div', class_=\"column middle\").find(\"a\")['href']\n",
        "            moreInfo = requests.get(contentURL).content\n",
        "            bs2 = BeautifulSoup(moreInfo, 'lxml')\n",
        "            content = bs2.find('div', class_=\"main-article-section\").find('div', id=\"article-phara2\").text.strip()\n",
        "            news.append([title, formatedDate.strftime(\"%B %d, %Y\"), content])\n",
        "        else:\n",
        "            reached = True\n",
        "            break\n",
        "\n",
        "    print(f\"Total News as of now: {len(news)}\")\n",
        "    if reached:\n",
        "        print(\"Reached the given date limit.\")\n",
        "        break\n",
        "\n",
        "print(\"Scraping completed.\")\n",
        "\n",
        "# Now you can process the 'news' list as needed, such as writing it to a CSV file.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "63W622Sisy8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Island news Scraping\n",
        "# Create an empty list to store the news data\n",
        "news_data = []\n",
        "\n",
        "for page in range(1, 401):  # Start from page 1\n",
        "    news_website_url = f\"https://island.lk/category/news/page/{page}/\"\n",
        "    html_text = requests.get(news_website_url).text\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    reports = soup.find_all('li', class_='mvp-blog-story-wrap left relative infinite-post')\n",
        "\n",
        "    for report in reports:\n",
        "        news_title = report.find('h2').text\n",
        "        news_url = report.find('a')['href']\n",
        "        individual_news = requests.get(news_url).text\n",
        "        soup2 = BeautifulSoup(individual_news, 'lxml')\n",
        "        news_date = soup2.find('span', class_='mvp-post-date updated').text\n",
        "        news_content = soup2.find('div', id='mvp-content-main').text.strip()\n",
        "\n",
        "        # Append the news data as a dictionary to the list\n",
        "        news_data.append({\n",
        "            'Title': news_title,\n",
        "            'Date': news_date,\n",
        "            'URL': news_url,\n",
        "            'Content': news_content\n",
        "        })\n",
        "\n",
        "    print(f\"Scraped page {page}\")\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'island_data.csv'\n",
        "\n",
        "# Write the news data to the CSV file\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Title', 'Date', 'URL', 'Content']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write each news item as a row\n",
        "    for news_item in news_data:\n",
        "        writer.writerow(news_item)\n",
        "\n",
        "print(\"CSV file has been created successfully.\")"
      ],
      "metadata": {
        "id": "FgqrHjuZtBAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dailynews Scraping\n",
        "# Create an empty list to store the news data\n",
        "news_data = []\n",
        "\n",
        "for page in range(1, 401):  # Start from page 1\n",
        "    news_website_url = f\"https://dailynews.lk/category/news/page/{page}/\"\n",
        "    html_text = requests.get(news_website_url).text\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    reports = soup.find_all('li', class_='mvp-blog-story-wrap left relative infinite-post')\n",
        "\n",
        "    for report in reports:\n",
        "        news_title = report.find('h2').text\n",
        "        news_url = report.find('a')['href']\n",
        "        individual_news = requests.get(news_url).text\n",
        "        soup2 = BeautifulSoup(individual_news, 'lxml')\n",
        "        news_date = soup2.find('span', class_='mvp-post-date updated').text\n",
        "        news_content = soup2.find('div', id='mvp-content-main').text.strip()\n",
        "\n",
        "        # Append the news data as a dictionary to the list\n",
        "        news_data.append({\n",
        "            'Title': news_title,\n",
        "            'Date': news_date,\n",
        "            'URL': news_url,\n",
        "            'Content': news_content\n",
        "        })\n",
        "\n",
        "    print(f\"Scraped page {page}\")\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = 'island_data.csv'\n",
        "\n",
        "# Write the news data to the CSV file\n",
        "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Title', 'Date', 'URL', 'Content']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write each news item as a row\n",
        "    for news_item in news_data:\n",
        "        writer.writerow(news_item)\n",
        "\n",
        "print(\"CSV file has been created successfully.\")"
      ],
      "metadata": {
        "id": "7gTU46BEQ5xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "news = []\n",
        "date_format = \"%B %d, %Y\"\n",
        "target_date_str = \"July 17, 2022\"\n",
        "target_date = datetime.strptime(target_date_str, date_format)\n",
        "reached = False\n",
        "\n",
        "print(target_date)\n",
        "\n",
        "for i in range(1,800):\n",
        "  print(f\"Page Number: {i}\")\n",
        "  url = f\"https://lankanewsweb.net/archives/category/news/page/{i}/\"\n",
        "  response = requests.get(url).content\n",
        "  bs = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "  listOfNews = bs.find('div', class_=\"td_block_wrap tdb_loop tdi_89 tdb-numbered-pagination td_with_ajax_pagination td-pb-border-top td_block_template_8 tdb-category-loop-posts\")\n",
        "  articles = listOfNews.find_all('div', class_=\"tdb_module_loop td_module_wrap td-animation-stack\")\n",
        "\n",
        "  for article in articles:\n",
        "    title = article.find('div', class_=\"td-module-meta-info\").find('p').text\n",
        "    contentURL = article.find('div', class_=\"td-module-meta-info\").find('p').find('a').attrs['href']\n",
        "\n",
        "    moreInfo = requests.get(contentURL).content\n",
        "    bs2 = BeautifulSoup(moreInfo, 'lxml')\n",
        "    dateString = bs2.find('div', class_=\"td-theme-wrap\").find('div', class_=\"td_block_wrap tdb_single_date tdi_97 td-pb-border-top td_block_template_1 tdb-post-meta\").find('div', class_=\"tdb-block-inner td-fix-index\").find('time').text.strip()\n",
        "    formatedDate = datetime.strptime(dateString, date_format)\n",
        "    print(formatedDate)\n",
        "    if(target_date < formatedDate):\n",
        "      content = bs2.find('div', class_=\"td-theme-wrap\").find('div', class_=\"vc_column_inner tdi_95 wpb_column vc_column_container tdc-inner-column td-pb-span9\").find('div', class_=\"vc_column-inner\").find('div', class_=\"wpb_wrapper\").find('div', class_=\"td_block_wrap tdb_single_content tdi_100 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\").find('div', class_=\"tdb-block-inner td-fix-index\").text.strip()\n",
        "      news.append([title, formatedDate.strftime(\"%B %d, %Y\"), content])\n",
        "    else:\n",
        "      reached = True\n",
        "\n",
        "  print(f\"Total News as of now: {len(news)}\")\n",
        "  if(reached):\n",
        "    print(\"Reached the give date limit.\")\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "_fAqvA3GtGQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "date_format = \"%Y-%m-%dT%H:%M:%S%z\"\n",
        "target_date_str = \"2022-07-18T00:00:00+00:00\"\n",
        "target_date = datetime.strptime(target_date_str, date_format)\n",
        "reached = False\n",
        "\n",
        "for i in range(1,400):\n",
        "  print(f\"Page Number: {i}\")\n",
        "  url = f\"https://www.newswire.lk/category/news/page/{i}/\"\n",
        "  response = requests.get(url).content\n",
        "  bs = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "  listOfNews = bs.find('div', id=\"content-wrap\")\n",
        "  articles = listOfNews.find_all('article')\n",
        "\n",
        "  for article in articles:\n",
        "    container = article.find('div', class_=\"entry-grid-content\")\n",
        "    time = container.find('div', class_=\"entry-byline\").find('time').attrs['datetime']\n",
        "    datetime_object = datetime.strptime(time, date_format)\n",
        "    dateStr = datetime_object.strftime(\"%B %d, %Y\")\n",
        "    # print(f\"Published Date: {dateStr}\")\n",
        "\n",
        "    if(target_date < datetime_object):\n",
        "      titleTag = container.find('header', class_=\"entry-header\").find('h2').find('a')\n",
        "      title = titleTag.text\n",
        "      moreURL = titleTag.attrs['href']\n",
        "      moreInfo = requests.get(moreURL).content\n",
        "      bs2 = BeautifulSoup(moreInfo, 'lxml')\n",
        "      content = bs2.find('div', class_=\"content-wrap theiaStickySidebar\").find('article').find(\"div\", class_=\"entry-the-content\").text\n",
        "      news.append([title, dateStr, content])\n",
        "      # print(f\"Total news until page {i} is {len(news)}\")\n",
        "    else:\n",
        "      reached = True\n",
        "\n",
        "  print(f\"Total News as of now: {len(news)}\")\n",
        "  if(reached):\n",
        "    print(\"Reached the give date limit.\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "fUMZOPqbtL3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = []\n",
        "date_format = \"%d %B %Y\"\n",
        "target_date_str = \"01 June 2022\"\n",
        "target_date = datetime.strptime(target_date_str, date_format)\n",
        "reached = False\n",
        "\n",
        "print(target_date)\n",
        "\n",
        "for i in range(0,200):\n",
        "  print(f\"Page Number: {i}\")\n",
        "  url = f\"https://www.tamilguardian.com/news-region/tamil-affairs?page={i}\"\n",
        "  response = requests.get(url).content\n",
        "  bs = BeautifulSoup(response, 'lxml')\n",
        "\n",
        "  listOfNews = bs.find('div', class_=\"view-content\")\n",
        "  articles = listOfNews.find_all('div', class_=\"views-row\")\n",
        "\n",
        "  for article in articles:\n",
        "    title = article.find('div', class_=\"title-body\").find('div', class_=\"post-title\").find('h2')\n",
        "    contentURL = title.find('a').attrs['href']\n",
        "    moreInfo = requests.get(f\"https://www.tamilguardian.com{contentURL}\").content\n",
        "    bs2 = BeautifulSoup(moreInfo, 'lxml')\n",
        "    dateString = bs2.find('div', class_=\"content-first\").find('div', class_=\"post-date-1\").text.strip()\n",
        "    formatedDate = datetime.strptime(dateString, date_format)\n",
        "    print(formatedDate)\n",
        "\n",
        "    if(target_date <= formatedDate):\n",
        "      content = bs2.find('div', class_=\"content-first\").find('div', class_=\"content\").find('div', class_=\"field-item even\").text.strip()\n",
        "      news.append([title.text.strip(), formatedDate.strftime(\"%B %d, %Y\"), content])\n",
        "    else:\n",
        "      reached = True\n",
        "      break\n",
        "\n",
        "  print(f\"Total News as of now: {len(news)}\")\n",
        "  if(reached):\n",
        "    print(\"Reached the give date limit.\")\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Odr6Dk2atRVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.dailymirror.lk\"\n",
        "categories = {\n",
        "    \"featured\": \"/features/131\",\n",
        "    \"news\": \"/news/209\",\n",
        "    \"financial\": \"/financial-news/265\",\n",
        "    \"other\": \"/Other/117\",\n",
        "    \"sports\": \"/sports\",\n",
        "    \"expose\": \"/expose/333\",\n",
        "    \"hardtalk\": \"/hard-talk/334\",\n",
        "    \"business\": \"/business-news/273\",\n",
        "}\n",
        "\n",
        "complete_article_list = []\n",
        "\n",
        "# loop through the categories\n",
        "for category_name, category_url in categories.items():\n",
        "    print(f\"Running for: {category_name} : {base_url}{category_url}\")\n",
        "    limit_reached = False\n",
        "    loop_count = 0\n",
        "    page_size = 30 # number of articles per page, this is fixed\n",
        "    article_list = []\n",
        "\n",
        "    while not limit_reached:\n",
        "        url = f\"{base_url}{category_url}/{loop_count * page_size}\"\n",
        "        print(f\"Scraping page #{loop_count + 1} from {url}\")\n",
        "        time.sleep(1)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        if(category_name != \"sports\"):\n",
        "            main_div =  soup.find('div', id='breakingnewsads')\n",
        "            articles = main_div.find_all('div', class_='lineg')\n",
        "        else:\n",
        "            main_div =  soup.find('div', class_='inleft')\n",
        "            # Find all the div elements with class \"lineg\" within the main div\n",
        "            articles = main_div.find_all('div', class_='row')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Check if the page has no articles\n",
        "        if len(articles) == 0:\n",
        "            limit_reached = True\n",
        "            break\n",
        "\n",
        "        for article in articles:\n",
        "            date_time_str = article.find('span', class_='gtime').text.strip()\n",
        "            date_time = datetime.strptime(date_time_str, '%d %b %Y')\n",
        "            date_time_iso = date_time.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
        "            if date_time <= datetime(2022, 6, 1):\n",
        "                limit_reached = True\n",
        "                break  # Break out of the loop\n",
        "\n",
        "            title = article.find('h3', class_='cat-hd-tx').text.strip()\n",
        "            excerpt = article.find_all('p')[1].text.strip()\n",
        "            url = article.select_one(\"a\")[\"href\"]\n",
        "\n",
        "            time.sleep(1)\n",
        "            # ignore article if url has \"https://www.dailymirror.lk/infographics\"\n",
        "            if \"https://www.dailymirror.lk/infographics\" in url:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"Scraping article #{url}\")\n",
        "                # get the full article content from the article url\n",
        "                article_response = requests.get(url)\n",
        "                article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
        "                article_content = article_soup.find('header', class_='inner-content').text.strip()\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping article #{url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "            article_dict = {\n",
        "                \"title\": title,\n",
        "                \"excerpt\": excerpt,\n",
        "                \"date_time\": date_time_iso,\n",
        "                \"url\": url,\n",
        "                \"content\": article_content,\n",
        "                \"category\": category_name,\n",
        "                \"source\": \"daily_mirror\"\n",
        "            }\n",
        "            article_list.append(article_dict)\n",
        "            complete_article_list.append(article_dict)\n",
        "        loop_count += 1\n",
        "    print(f\"Total articles scraped for {category_name}: {len(article_list)}\")\n",
        "    with open(f\"datasets/daily_mirror/{category_name}.json\", \"w\") as file:\n",
        "        json.dump(article_list, file)\n",
        "\n",
        "\n",
        "print(f\"Total articles scraped: {len(article_list)}\")\n",
        "\n",
        "# Save the articles as a JSON array\n",
        "with open(\"datasets/daily_mirror.json\", \"w\") as file:\n",
        "    json.dump(complete_article_list, file)\n",
        "\n",
        "print(\"Data collection completed for all categories.\")\n",
        "\n",
        "# read datasets/daily_mirror.json and print number of articles collected\n",
        "with open(f'datasets/daily_mirror.json', 'r') as f:\n",
        "    daily_mirror_data = json.load(f);\n",
        "\n",
        "print(f'Number of news items collected from daily mirror: {len(daily_mirror_data)}')"
      ],
      "metadata": {
        "id": "zbx12F92tWm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://adaderana.lk/news.php?nid={id}\"\n",
        "start_id = 82795\n",
        "batch_size = 100\n",
        "page = 1\n",
        "articles = []\n",
        "\n",
        "\n",
        "def parse_datetime(datetime_str):\n",
        "    # Parse the date and time string into the desired format: \"2022-06-01T07:42:04.000Z\"\n",
        "    datetime_obj = datetime.strptime(datetime_str, \"%B %d, %Y   %I:%M %p\")\n",
        "    datetime_obj -= timedelta(hours=5, minutes=30)  # Convert to Sri Lankan time (GMT +5:30)\n",
        "    formatted_datetime = datetime_obj.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
        "    return formatted_datetime\n",
        "\n",
        "\n",
        "def scrape_article(article_id):\n",
        "    url = base_url.format(id=article_id)\n",
        "    # print(f\"Scraping article #{article_id} from {url}\")\n",
        "    headers = {\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,/;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
        "        \"Cache-Control\": \"max-age=0\",\n",
        "        \"Cookie\": \"\",\n",
        "        \"Sec-Ch-Ua\": '\"Not.A/Brand\";v=\"8\", \"Chromium\";v=\"114\", \"Google Chrome\";v=\"114\"',\n",
        "        \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
        "        \"Sec-Ch-Ua-Platform\": '\"macOS\"',\n",
        "        \"Sec-Fetch-Dest\": \"document\",\n",
        "        \"Sec-Fetch-Mode\": \"navigate\",\n",
        "        \"Sec-Fetch-Site\": \"none\",\n",
        "        \"Sec-Fetch-User\": \"?1\",\n",
        "        \"Upgrade-Insecure-Requests\": \"1\",\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            title = soup.select_one(\"h1\").text.strip()\n",
        "            datetime_str = soup.select_one(\".news-datestamp\").text.strip()\n",
        "            content_element = soup.select_one(\".news-content\")\n",
        "            content_html = str(content_element)\n",
        "            if(content_html == \"<div class=\\\"news-content\\\">\\n</div>\"):\n",
        "                return False\n",
        "            parsed_datetime = parse_datetime(datetime_str)\n",
        "            article = {\n",
        "                \"id\": article_id,\n",
        "                \"title\": title,\n",
        "                \"datetime\": parsed_datetime,\n",
        "                \"content\": content_html,\n",
        "                \"url\": url,\n",
        "                \"source\": \"adaderana\"\n",
        "            }\n",
        "            articles.append(article)\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping article #{article_id}: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "while True:\n",
        "    if not scrape_article(start_id):\n",
        "        print(f\"Could not scrape article #{start_id}. Stopping.\")\n",
        "        print(f\"Total articles scraped: {len(articles)}\")\n",
        "        break\n",
        "    start_id += 1\n",
        "    # Delay for 1 seconds\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save the articles as a JSON array\n",
        "with open(\"datasets/adaderana.json\", \"w\") as file:\n",
        "    json.dump(articles, file)\n",
        "\n",
        "\n",
        "print(f\"Scraped {len(articles)} articles from Ada Derana.\")"
      ],
      "metadata": {
        "id": "A8shAUpFtoCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}